{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213549e2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-03T07:45:00.278320Z",
     "iopub.status.busy": "2024-03-03T07:45:00.277850Z",
     "iopub.status.idle": "2024-03-03T07:45:01.320224Z",
     "shell.execute_reply": "2024-03-03T07:45:01.318732Z"
    },
    "papermill": {
     "duration": 1.050379,
     "end_time": "2024-03-03T07:45:01.323167",
     "exception": false,
     "start_time": "2024-03-03T07:45:00.272788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164011f4",
   "metadata": {
    "papermill": {
     "duration": 0.002183,
     "end_time": "2024-03-03T07:45:01.328188",
     "exception": false,
     "start_time": "2024-03-03T07:45:01.326005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Word embeddings\n",
    "Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. Unlike traditional one-hot encoding, where words are represented as isolated units, embeddings capture the semantic relationships between words by placing semantically similar words closer together in the embedding space. This results in a dense representation of words, where each word is represented by a real-valued vector with several dimensions (typically hundreds or thousands).\n",
    "\n",
    "#### Steps Involved and Uses:\n",
    "Preprocessing Text: The first step involves cleaning and preparing the text data. This may include removing punctuation, converting text to lowercase, tokenizing (splitting text into words or tokens), and possibly removing stop words (common words that are often filtered out).\n",
    "\n",
    "Choosing Vocabulary Size: Decide on the vocabulary size, which includes determining the set of words that will be represented in the embedding space. This often involves selecting the most frequent words in the dataset.\n",
    "\n",
    "Word Encoding: Each word in the vocabulary is assigned a unique integer ID. This can be done using methods like one-hot encoding initially before being converted into dense vectors through embeddings.\n",
    "\n",
    "Training the Embedding Layer: The embedding vectors can be learned in two ways: by training a model on a specific task (such as text classification or sentiment analysis) and learning the embeddings within this model, or by using pre-trained embeddings obtained by training on large text corpora like Google News or Wikipedia.\n",
    "\n",
    "Using Embeddings: Once trained, these embeddings can be used as input to various NLP tasks, such as text classification, sentiment analysis, machine translation, and more. The embeddings capture semantic meanings, allowing the model to understand context and similarity between words.\n",
    "\n",
    "#### Example with the Sentence \"I like you\":\n",
    "Suppose each word in the sentence \"I like you\" is represented in a 3-dimensional embedding space. After preprocessing and assigning an integer ID to each unique word, the embeddings for these words are learned or retrieved from a pre-trained set. Instead of representing \"I\", \"like\", and \"you\" as one-hot vectors (which would be sparse and high-dimensional), each word is represented as a dense vector, e.g.,\n",
    "\n",
    "\"I\" might be represented as [0.5, 0.1, -0.4],\n",
    "\"like\" as [0.7, -0.3, 0.2],\n",
    "\"you\" as [0.4, 0.9, -0.5].\n",
    "These vectors capture more information about each word, including semantic relationships to other words.\n",
    "\n",
    "#### Advantages:\n",
    "Semantic Meaning: Word embeddings capture the semantic relationships between words, meaning that words with similar meanings are closer in the vector space. This allows models to better understand context and nuances in language.\n",
    "\n",
    "Efficiency: Embeddings provide a dense, low-dimensional representation of words, which is more efficient than high-dimensional sparse vectors like one-hot encodings. This efficiency translates to models that are faster to train and have better performance.\n",
    "\n",
    "#### Limitations:\n",
    "Out-of-Vocabulary (OOV) Words: Words not seen during training (OOV words) are not represented in the embedding space. This can limit the model's ability to understand and process new or rare words.\n",
    "\n",
    "Static Representations: Traditional word embeddings provide a static representation for each word, meaning that the context in which a word is used is not considered. This can be a limitation for words with multiple meanings based on context (polysemy). However, more recent models like BERT and ELMo attempt to address this by providing context-dependent embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ed6d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:45:01.335602Z",
     "iopub.status.busy": "2024-03-03T07:45:01.334886Z",
     "iopub.status.idle": "2024-03-03T07:45:17.613718Z",
     "shell.execute_reply": "2024-03-03T07:45:17.612351Z"
    },
    "papermill": {
     "duration": 16.285937,
     "end_time": "2024-03-03T07:45:17.616608",
     "exception": false,
     "start_time": "2024-03-03T07:45:01.330671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 07:45:03.670461: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-03 07:45:03.670622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-03 07:45:03.839456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.15.0\n",
      "Padded Sequences:\n",
      " [[  0   0   0   0 430 463 110 311]\n",
      " [  0   0   0   0 430 463 110 390]\n",
      " [  0   0   0   0 430 173 110 419]\n",
      " [  0   0   0 436 185  11 314 277]\n",
      " [  0   0   0 436 185  11 314  88]\n",
      " [  0   0   0  74 430 332 110 467]\n",
      " [  0   0   0   0 469 230 354 314]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "Embedding for the first sentence:\n",
      " [[0.5006255]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Embeddings for all sentences:\n",
      " [[0.5006255 ]\n",
      " [0.4971775 ]\n",
      " [0.4944139 ]\n",
      " [0.49560043]\n",
      " [0.49277392]\n",
      " [0.49107322]\n",
      " [0.48755872]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2  # Example regularizer\n",
    "\n",
    "# Check TensorFlow version (Ensure it's > 2.0)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Define sentences to be encoded and embedded\n",
    "sentences = [\n",
    "    'the glass of milk',\n",
    "    'the glass of juice',\n",
    "    'the cup of tea',\n",
    "    'I am a good boy',\n",
    "    'I am a good developer',\n",
    "    'understand the meaning of words',\n",
    "    'your videos are good'\n",
    "]\n",
    "\n",
    "# Specify the vocabulary size\n",
    "vocabulary_size = 500\n",
    "\n",
    "# Function to perform one-hot encoding on the sentences\n",
    "def encode_sentences(sentences, vocab_size):\n",
    "    return [one_hot(sentence, vocab_size) for sentence in sentences]\n",
    "\n",
    "# Encode the sentences\n",
    "one_hot_encoded = encode_sentences(sentences, vocabulary_size)\n",
    "\n",
    "# Pre-pad the sequences to ensure uniform length\n",
    "sequence_length = 8\n",
    "padded_sequences = pad_sequences(one_hot_encoded, padding='pre', maxlen=sequence_length)\n",
    "print(\"Padded Sequences:\\n\", padded_sequences)\n",
    "\n",
    "# Define embedding dimensions\n",
    "embedding_dim = 10\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocabulary_size,\n",
    "        output_dim=embedding_dim,\n",
    "        embeddings_initializer=\"uniform\",  # Default, but specified for clarity\n",
    "        embeddings_regularizer=l2(0.01),  # Example L2 regularization\n",
    "        # embeddings_constraint= <YourConstraintHere>, Optional: specify if needed\n",
    "        # mask_zero=True, Uncomment if you are going to use RNNs and need to mask zero padding\n",
    "    ),\n",
    "    Flatten(),  # Flatten the output for Dense layer\n",
    "    Dense(1, activation='sigmoid')  # Example of adding a Dense layer for a potential classification task\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')  # Assuming a binary classification task\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Example: Predict embeddings for the first sentence\n",
    "first_sentence_embedding = model.predict(padded_sequences[0].reshape(1, -1))\n",
    "print(\"Embedding for the first sentence:\\n\", first_sentence_embedding)\n",
    "\n",
    "# Example: Predict embeddings for all sentences\n",
    "all_embeddings = model.predict(padded_sequences)\n",
    "print(\"Embeddings for all sentences:\\n\", all_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23.274097,
   "end_time": "2024-03-03T07:45:20.199794",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-03T07:44:56.925697",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
